{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lexsub: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=27.89\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Default program finds the topn most similar words to a target word from the provided GloVe vector set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Provided Glove vector was bad to find synonyms of a target word for each sentences; therefore, we decided to implement baseline first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lexsub: baseline program(single synonym graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexsub_base import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the baseline solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english edge position line point place way while back front\n",
      "english edge position line point place way while back front\n",
      "english edge position line point place way while back front\n",
      "english edge position line point place way while back front\n",
      "english edge position line point place way while back front\n",
      "english edge position line point place way while back front\n",
      "english edge position line point place way while back front\n",
      "english edge position line point place way while back front\n",
      "english edge position line point place way while back front\n",
      "english edge position line point place way while back front\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.retrofit.magnitude'))\n",
    "lexsub_old = LexSub(os.path.join('data','glove.6B.100d.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split(), lexsub_old)))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the baseline output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=50.03\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "we used Wordnet synonym text file for our synonym set. We found from paper that the score is the best when the weights are: Beta = 1 and Alpha = (number of synonyms) * k, so we adopted this. The iteration T was set to 25. For the synonym graph, we used single synonym graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building full synonym graph\n",
    "\n",
    "full undirected graph is having edges for each synonyms for every single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = line.lower().strip().split(sep)\n",
    "            \n",
    "synonyms = [filterWords(i) for i in synonyms]\n",
    "          \n",
    "############## using full graph\n",
    "or i in range(0, len(synonyms)):\n",
    "    if (synonyms[i] not in synonymSets):\n",
    "        synonymSets[synonyms[i]] = set()\n",
    "    synonymSets[synonyms[i]].update(synonyms[:i]+synonyms[i+1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building single synonym graph\n",
    "\n",
    "single undirected graph is having edges from first word to other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = line.lower().strip().split(sep)\n",
    "            \n",
    "synonyms = [filterWords(i) for i in synonyms]\n",
    "\n",
    "if (synonyms[0] not in synonymSets):\n",
    "    synonymSets[synonyms[0]] = set()\n",
    "synonymSets[synonyms[0]].update(synonyms[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Weights according to the number of synonyms of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "beta = 1\n",
    "k = 0.5\n",
    "\n",
    "oldWordVector = oldWordVectors.query(word)\n",
    "vectorSum = reduce(lambda x,y:x+y, (wordVectors[synonym]*beta for synonym in synonymSets[word] if synonym in wordVectors), array([0]*oldWordVector.ndim))\n",
    "synonymCount = sum(1 for synonym in synonymSets[word] if synonym in wordVectors)\n",
    "            \n",
    "if synonymCount > 0:\n",
    "    try:\n",
    "        alpha = beta*synonymCount * k\n",
    "        vectorSum += oldWordVector * (alpha) \n",
    "                    \n",
    "        vectorSum = vectorSum/(synonymCount*beta+alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches\n",
    "\n",
    "1. adjusting weights(alpha, beta, and k) and T according to the length of edges. <br>\n",
    "2. testing two different styles of graphs(full and single)<br>\n",
    "3. using different lexicons(wordnet, wordnet+, ppdb, and framenet) or combinations(ex. ppdb + wordnet)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Adjusting T and weights(alpha, beta, and k) was slightly effective. We tried to merge synonym files (ppdb, wordnet, wordnet+, and framenet) into one synonym set, but the result was not good. Using only one wordnet text file had the best result, so we decided to use wordnet txt file only. We tried various combinations of weights and confirmed that the beta =1 and alpha = (number of the synonyms) * 0.5 is the best. For the synonym graph, we tested two graph styles which are single and full undirected grpah, and single graph had better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lexsub: context substitution program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexsub_con import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the context substitution solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal english draw match place bottom corner back edge away\n",
      "english edge position line point place way while back front\n",
      "english view perspective point middle way part place edge the\n",
      "english edge position line point place way while back front\n",
      "english edge position line point place way while back front\n",
      "edge back turn way line look right turning shoes going\n",
      "can means stand proper place they only turn rather way\n",
      "english edge position line point place way while back front\n",
      "english edge position line point place way while back front\n",
      "along near line part located edge middle area the where\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.retrofit.magnitude'), 100)\n",
    "lexsub_old = LexSub(os.path.join('data','glove.6B.100d.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split(), lexsub_old)))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the context substitution output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=42.51\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "if len(ref_data) == len(output):\n",
    "    print(\"Score={:.2f}\".format(100*precision(ref_data, output)))\n",
    "else:\n",
    "    print(\"length error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "From the baseline, lexical substitution gets 100 candidates. This program calculates scores using context words and target words on the GloVe and retrofitted vector field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting  context words\n",
    "\n",
    "setence_range parameter decides w(i-r)-w(i-1) and (wi+1)-w(i+r) as the context words where i is index(w(i) is target word) and r is range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_range = 1 ##around words from index\n",
    "for word in sentence[index-sentence_range:index]:\n",
    "    if word in oldvec.wvecs_dict and word.isalpha() and word not in self.non_context:\n",
    "        context_words.append(oldvec.wvecs_dict[word])\n",
    "for word in sentence[index+1:index+sentence_range+1]:\n",
    "    if word in oldvec.wvecs_dict and word.isalpha() and word not in self.non_context:\n",
    "        context_words.append(oldvec.wvecs_dict[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Calculation: Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal = len(context_words)\n",
    "if context_words:\n",
    "    for word in candidates:\n",
    "        candidates_dict[word] = sum((np.linalg.norm(oldvec.wvecs_dict[word] - c)) for c in context_words))\n",
    "    else:\n",
    "        return candidates[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Calculation: add(cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal = len(context_words)\n",
    "if context_words:\n",
    "    for word in candidates:\n",
    "        candidates_dict[word] = ((1-spatial.distance.cosine(self.wvecs_dict[word], v)) + \n",
    "                                 sum((1-spatial.distance.cosine(oldvec.wvecs_dict[word], c)) \n",
    "                                     for c in context_words)) / (bal+1)\n",
    "    else:\n",
    "        return candidates[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Calculation: baladd(cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal = len(context_words)\n",
    "if context_words:\n",
    "    for word in candidates:\n",
    "        candidates_dict[word] = ((1-spatial.distance.cosine(self.wvecs_dict[word], v)) * bal + \n",
    "                                 sum((1-spatial.distance.cosine(oldvec.wvecs_dict[word], c)) \n",
    "                                     for c in context_words)) / (bal*2)\n",
    "    else:\n",
    "        return candidates[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of non context words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_context = [\"it's\", \"she's\", 'were', 'because', 'this', 'couldn', 'then', 'how', 'd', 'doesn', 'down', 's', \n",
    "               'they', 'she', \"needn't\", 'wasn', 'haven', 'between', \"wouldn't\", 'the', 'ma', \"wasn't\", 'until', \n",
    "               'my', 'himself', \"that'll\", 'by', 'about', 'in', \"aren't\", \"should've\", 'why', 'nor', 'before', \n",
    "               'when', 'we', 'here', 'only', \"couldn't\", 'ain', 'no', 'your', 'will', 'own', 'his', \"you'll\", \n",
    "               'are', 'and', 'most', 'do', 'now', \"isn't\", 'having', 'on', 'her', 'theirs', 'under', 'with', 'to', \n",
    "               \"mightn't\", 'while', 'its', 'be', 'll', 'don', 'over', 'again', 'their', 'won', 'too', 'during', \n",
    "               'shan', 'herself', 'has', 'or', 'from', 'ours', 'into', 'our', 'above', 'wouldn', 'you', 'of', 'so', \n",
    "               't', 'he', 'doing', 'as', 'i', 'can', 'shouldn', 'have', 'at', 'other', 'hasn', 'more', 'yourselves', \n",
    "               'y', 'yours', 'very', 'themselves', 'which', 'these', 'being', 'both', 'aren', 'did', 'than', 'needn', \n",
    "               'for', 'itself', \"haven't\", 'through', 'weren', 'but', 'once', 'isn',  'ourselves', 'didn', 'not', \n",
    "               'yourself', 'mightn', 'after', 've', 'him', 'whom', \"hasn't\", 'a', 'hadn', \"shouldn't\", \"mustn't\", \n",
    "               'those', 'off',  'each', 'was', \"didn't\", \"you'd\", 'where', 'o', 'further', 'below', \"shan't\",  \n",
    "               'myself', 'mustn', 'is', 'been', 'just', 'any', 'out', 'that', 'm', 'such',  'me', 'same', 'hers', \n",
    "               'some', 'had', 'does', 'against', 'should', \"you've\",  \"doesn't\", \"you're\", 'them', 'am', 'if', \n",
    "               'who', 'few', 'what', 'there',  \"don't\", \"weren't\", \"won't\", 'an', 'all', 're', 'it', 'up', \"hadn't\",\n",
    "               \"'ll\", ',', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches\n",
    "\n",
    "1. changing sentence ranges <br>\n",
    "2. testing various score calculations(distance, add, and baladd)<br>\n",
    "3. adjusting number of candidates<br>\n",
    "4. excluding non context words from the context words list<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Using context words to evaluate synonyms always had bad accuracies than without it. We tried to add whole words in the sentence as context words, but the score was extremely lowered. We could have the best score with range 1. When we excluded non context words(like: it, as, a, the, and so on) from the context words list, the score was higher. Having too many candidates increased the randomness; therefore, adjusted it to 100 which is 20 times of the topn. There were various score methods in the paper, and baladd had the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
