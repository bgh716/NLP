\documentclass[letter]{article}
% Set target color model to RGB
\usepackage[inner=2.0cm,outer=2.0cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{setspace}
\usepackage[rgb]{xcolor}
\usepackage{verbatim}
\usepackage{subcaption}
\usepackage{amsgen}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{amsbsy}
\usepackage{amsopn}
\usepackage{tikz}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage[colorlinks=true, urlcolor=blue,  linkcolor=blue, citecolor=blue]{hyperref}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

%\usetikzlibrary{through,backgrounds}
\hypersetup{%
pdfauthor={Angel Xuan Chang},%
pdftitle={Homework},%
pdfkeywords={Tikz,latex,bootstrap,uncertaintes},%
pdfcreator={PDFLaTeX},%
pdfproducer={PDFLaTeX},%
}
%\usetikzlibrary{shadows}
% \usepackage[francais]{babel}
\usepackage{booktabs}
% \input{macros.tex}

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{defn}[thm]{Definition}
\newtheorem{rem}[thm]{Remark}
\numberwithin{equation}{section}

\newcommand{\homework}[6]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CMPT 413/713:~Natural Language Processing \hfill {\small (#2)}} }
       \vspace{6mm}
       \hbox to 6.28in { {\Large \hfill #1  \hfill} }
       \vspace{6mm}
       \hbox to 6.28in {  {\it Group Members: {\rm #3} } \hfill }
	\hbox to 6.28in { { \it Group Name: {\rm #4} } \hfill}
      \vspace{2mm}}
   }
   \end{center}
   \markboth{#4 -- #1}{#4 -- #1}
   \vspace*{4mm}
}

\newcommand{\problem}[2]{~\\\fbox{\textbf{Problem #1}}\newline\newline}
\newcommand{\subproblem}[1]{~\newline\textbf{(#1)}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\Hy}{\mathcal{H}}
\newcommand{\VS}{\textrm{VS}}

\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbX}{\mathbb{X}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\0}{\mathbf{0}}
\newcommand{\code}[1]{\texttt{#1}}


\begin{document}
\homework{Programming Homework \#4}{Due: 11/09/21}{Joshua Malmberg, Hewei Cao, Keenan Byun}{Mochi}


\section{Objective}

	Machine translation is a sequence-to-sequence natural language processing task involving converting a sentence in one language to another. In this assignment, we are provided a pretrained neural machine translation model and given the task of completing the implantation of attention model and decoding algorithm.\newline

\section{Method}

	We first implemented a baseline model, using the pretrained sequence-to-sequence model, the intention model prescribed in the assignment, and a greedy decoder algorithm. We then implemented two ad hoc methods to improve the default model, unknown word replacement and repetition penalties. Finally, we implemented a beam search decoder algorithm and compared its performance with the greedy decoding algorithm.


\section{Results}

\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
   & Dev Score  \\ 
\hline
Baseline & 17.1139  \\  
Baseline w/ Ad Hoc Methods & 18.2037 \\
Beam Search w/ Ad Hoc Methods & 17.9264 \\
 \hline
\end{tabular}
\end{center}

	After implementing the baseline model, we observed several reccuring issues. First, <unk> tokens commonly appear in the output of our model, sometimes corresponding to proper nouns that do not require translation. Second, the decoder often predicts the correct word but repeats it multiple time, resulting in repeated words in the translation. Finally, we found that common words such as “a”, “and”, “so”, etc. often are incorrectly generated by the decoder, despite never occuring in the original sentence.
	
	We implemented ad hoc methods to address these issues. To prevent repeated words and the seemingly random generation of common words, we placed penalties whenever either occured. To improve performance for out-of-vocabulary words, we replaced out-of-vocabulary words with the word in the source language, rather than using the unknown token. These methods provided an appreciable improvement over the baseline method, however these problems eliminated entirely. 
	
	Despite beam search typically providing better performance than greedy decoding, we found that combined with our ad hoc methods beam search performed worse than the baseline. This was surprising, as greedy decoding is a special case of beam search and as such should not provide superior performance. 
\section{Contributions}

\textbf{Hewei Cao:} Implemented unknown word replacement and improved attention mechanism; \textbf{Keenan Byun:} Implemented baseline model, penalty for repeated words, and beam search; \textbf{Joshua Malmberg:} Wrote and typeset report and attempted implementation of ensemble decoding.

\end{document}
