{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zhsegment: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签 订 高 科 技 合 作 协 议\n",
      "新 华 社 上 海 八 月 三 十 一 日 电 （ 记 者 白 国 良 、 夏 儒 阁 ）\n",
      "“ 中 美 合 作 高 科 技 项 目 签 字 仪 式 ” 今 天 在 上 海 举 行 。\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"data/count_1w.txt\"))\n",
    "segmenter = Segment(Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.2675962075\n"
     ]
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.10f}\".format(tally), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Default solution segments every single characters in the text into single words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Every single characters were segmented in the text. Of course, the F-score was very low. We decided to try the given baseline algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zhsegment: baseline program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhsegment_baseline import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the baseline solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签订 高 科技 合作 协议\n",
      "新华社 上海 八月 三十一日 电 （ 记者 白 国 良 、 夏儒阁 ）\n",
      "“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data1=datafile(\"data/count_1w.txt\"), data2=datafile(\"data/count_2w.txt\"))\n",
    "segmenter = Segment(Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the baseline output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.8053163355\n"
     ]
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.10f}\".format(tally), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "In the middle of implementation of baseline, we found a problem which is the baseline algorithm does not handle the unknown words in the text. We made the unknown words to be independent words, and pushed the unknown word entry in the heap again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "F-score was incresed significantly, but we observed that when there are 2 unknown words in a sequence, they are segmented in our code. We decided to implement bigram method with smoothing while making the unknown words in a sequence into a single word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zhsegment: bigram program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhsegment_bigram import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the bigram solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原 总 兵 力 为 １１ · ６ 万余 人 ， 其中 在 拉脱维亚 驻军 最 多 ， 为 ５ 万余 人 ， 立陶宛 为 ３ · ５ 万余 人 ， 爱沙尼亚 为 ３ 万余 人 。\n",
      "原 苏联 解体 后 ， 该 军队 集 群转归 俄罗斯 所有 。\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data1=datafile(\"data/count_1w.txt\"), data2=datafile(\"data/count_2w.txt\"))\n",
    "segmenter = Segment(Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[318:320])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the bigram output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.8948552151\n"
     ]
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.10f}\".format(tally), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "F-score is about 90 now. We used biram probability to compute and compare each entry. For the smoothing, we implemented JM(linear interpolation) smoothing. Unknown words sequence is now considered as one word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We found that the numbers with unknown words are still segemented, so we forced them to be one word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zhsegment: final program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhsegment_rec import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the final solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原 总 兵 力 为 １１·６万 余 人 ， 其中 在 拉脱维亚 驻军 最 多 ， 为 ５万 余 人 ， 立陶宛 为 ３·５万 余 人 ， 爱沙尼亚 为 ３万 余 人 。\n",
      "原 苏联 解体 后 ， 该 军队 集 群转归 俄罗斯 所有 。\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data1=datafile(\"data/count_1w.txt\"), data2=datafile(\"data/count_2w.txt\"))\n",
    "segmenter = Segment(Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[318:320])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the bigram output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.8961249033\n"
     ]
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.10f}\".format(tally), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "The score was increased slightly, but now the numbers and unknown words combinded each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "When the word is unique word, program segments the word into single characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
