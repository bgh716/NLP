{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chunker: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1027/1027 [00:02<00:00, 459.66it/s]\n"
     ]
    }
   ],
   "source": [
    "chunker = LSTMTagger(os.path.join('data', 'train.txt.gz'), os.path.join('data', 'chunker'), '.tar')\n",
    "decoder_output = chunker.decode('data/input/dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 23663 tokens with 11896 phrases; found: 11672 phrases; correct: 8568.\n",
      "accuracy:  84.35%; (non-O)\n",
      "accuracy:  85.65%; precision:  73.41%; recall:  72.02%; FB1:  72.71\n",
      "             ADJP: precision:  36.49%; recall:  11.95%; FB1:  18.00  74\n",
      "             ADVP: precision:  71.36%; recall:  39.45%; FB1:  50.81  220\n",
      "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  70.33%; recall:  76.80%; FB1:  73.42  6811\n",
      "               PP: precision:  92.40%; recall:  87.14%; FB1:  89.69  2302\n",
      "              PRT: precision:  65.00%; recall:  57.78%; FB1:  61.18  40\n",
      "             SBAR: precision:  84.62%; recall:  41.77%; FB1:  55.93  117\n",
      "               VP: precision:  63.66%; recall:  58.25%; FB1:  60.83  2108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(73.40644276901988, 72.02420981842637, 72.70875763747455)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_output = [ output for sent in decoder_output for output in sent ]\n",
    "import conlleval\n",
    "true_seqs = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    for sent in conlleval.read_file(r):\n",
    "        true_seqs += sent.split()\n",
    "conlleval.evaluate(true_seqs, flat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "This documentation contains different methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Method 1\n",
    "final score 75.9. lower than method2, so we didn't spend much time on improve this code.</br>\n",
    "In below code, we concatenates the word embedding with the char-level-representation. </br>\n",
    "This code is written in LSTMTaggerModel.forward</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "embeds = self.word_embeddings(sentence)\n",
    "\n",
    "char_level_rep = []\n",
    "for w_ix in sentence:\n",
    "    word = self.ix_to_word[w_ix.item()]\n",
    "    v1 = [0]*100\n",
    "    v2 = [0]*100\n",
    "    v3 = [0]*100\n",
    "    v1[string.printable.index(word[0])] = 1\n",
    "    v3[string.printable.index(word[-1])] = 1\n",
    "    if(len(word)>2):\n",
    "        for i in range(1,len(word)):\n",
    "            ind = string.printable.index(word[i])\n",
    "            v2[ind] = v2[ind]+1\n",
    "    char_level_rep.append(v1+v2+v3)\n",
    "char_level_rep = torch.tensor(char_level_rep)\n",
    "\n",
    "embeds = torch.cat([embeds, char_level_rep], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the method1 output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processed 23663 tokens with 11896 phrases; found: 11836 phrases; correct: 9010.</br>\n",
    "accuracy:  86.14%; (non-O)</br>\n",
    "accuracy:  87.24%; precision:  76.12%; recall:  75.74%; FB1:  75.93</br>\n",
    "             ADJP: precision:  40.00%; recall:  15.93%; FB1:  22.78  90</br>\n",
    "             ADVP: precision:  68.98%; recall:  47.49%; FB1:  56.25  274</br>\n",
    "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0</br>\n",
    "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0</br>\n",
    "               NP: precision:  73.84%; recall:  79.41%; FB1:  76.52  6708</br>\n",
    "               PP: precision:  91.91%; recall:  87.96%; FB1:  89.89  2336</br>\n",
    "              PRT: precision:  58.70%; recall:  60.00%; FB1:  59.34  46</br>\n",
    "             SBAR: precision:  82.35%; recall:  47.26%; FB1:  60.05  136</br>\n",
    "               VP: precision:  68.83%; recall:  67.10%; FB1:  67.96  2246</br>\n",
    "(76.1236904359581, 75.73974445191661, 75.93123209169053)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Method 2\n",
    "final score 75.1337. </br>\n",
    "In below code, we used second RNN to encode char-level-representation, and concatenated the result of 64dim encoding to word embeddings as input of chunker RNN.</br>\n",
    "This code is written in LSTMTaggerModel.forward</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "embeds = self.word_embeddings(sentence)\n",
    "char_level_rep = []\n",
    "for w_ix in sentence:\n",
    "    word = self.ix_to_word[w_ix.item()]\n",
    "    v1 = [0.]*100\n",
    "    v2 = [0.]*100\n",
    "    v3 = [0.]*100\n",
    "    v1[string.printable.index(word[0])] = 1.\n",
    "    v3[string.printable.index(word[-1])] = 1.\n",
    "    #if(len(word)>2):\n",
    "    for i in range(0,len(word)):\n",
    "        ind = string.printable.index(word[i])\n",
    "        v2[ind] = v2[ind]+1\n",
    "    char_level_rep.append(v1+v2+v3)\n",
    "char_level_rep = torch.tensor(char_level_rep)\n",
    "        \n",
    "clr_lstm_out, _ = self.clr_lstm(char_level_rep.view(len(sentence), 1, -1))\n",
    "clr_lstm_out = clr_lstm_out.reshape((clr_lstm_out.shape[0],self.hidden_dim))\n",
    "embeds = torch.cat([embeds, clr_lstm_out], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the method2 output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processed 23663 tokens with 11896 phrases; found: 11843 phrases; correct: 8918.</br>\n",
    "accuracy:  85.61%; (non-O)</br>\n",
    "accuracy:  86.80%; precision:  75.30%; recall:  74.97%; FB1:  75.13</br>\n",
    "             ADJP: precision:  47.13%; recall:  18.14%; FB1:  26.20  87</br>\n",
    "             ADVP: precision:  68.15%; recall:  42.46%; FB1:  52.32  248</br>\n",
    "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0</br>\n",
    "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0</br>\n",
    "               NP: precision:  72.54%; recall:  79.22%; FB1:  75.74  6811</br>\n",
    "               PP: precision:  91.57%; recall:  86.28%; FB1:  88.84  2300</br>\n",
    "              PRT: precision:  74.29%; recall:  57.78%; FB1:  65.00  35</br>\n",
    "             SBAR: precision:  86.96%; recall:  42.19%; FB1:  56.82  115</br>\n",
    "               VP: precision:  68.31%; recall:  66.62%; FB1:  67.46  2247</br>\n",
    "dev.out score: 75.1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We found that increasing of accuracy of (non-O) improves the entire accuracy of the program; therefore, we focused on fixing misspelled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches\n",
    "1. make new vector 'v4' for contain vowls</br>\n",
    "2. use smoothing with weights unknow: +1 and known: +10</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Method 2 with approaches\n",
    "final score 76.9848. </br>\n",
    "In below code, we implemented approaches above on the method2.</br>\n",
    "This code is written in LSTMTaggerModel.forward</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "embeds = self.word_embeddings(sentence)\n",
    "char_level_rep = []\n",
    "vowls= ['a','e','i','o','u']\n",
    "for w_ix in sentence:\n",
    "    word = self.ix_to_word[w_ix.item()]\n",
    "    v1 = [1.]*100\n",
    "    v2 = [1.]*100\n",
    "    v3 = [1.]*100\n",
    "    v4 = [1.]*100\n",
    "    v1[string.printable.index(word[0])] = 10.\n",
    "    v3[string.printable.index(word[-1])] = 10.\n",
    "    #if(len(word)>2):\n",
    "    for i in range(0,len(word)):\n",
    "        ind = string.printable.index(word[i])\n",
    "        if i in vowls:\n",
    "            v2[ind] = v2[ind]+10\n",
    "        else:\n",
    "            v4[ind] = v4[ind]+10\n",
    "    char_level_rep.append(v1+v2+v3+v4)\n",
    "char_level_rep = torch.tensor(char_level_rep)\n",
    "        \n",
    "clr_lstm_out, _ = self.clr_lstm(char_level_rep.view(len(sentence), 1, -1))\n",
    "clr_lstm_out = clr_lstm_out.reshape((clr_lstm_out.shape[0],self.hidden_dim))\n",
    "embeds = torch.cat([embeds, clr_lstm_out], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the method2 output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processed 23663 tokens with 11896 phrases; found: 12036 phrases; correct: 9212. </br>\n",
    "accuracy:  86.61%; (non-O)</br>\n",
    "accuracy:  87.76%; precision:  76.54%; recall:  77.44%; FB1:  76.98</br>\n",
    "             ADJP: precision:  41.07%; recall:  20.35%; FB1:  27.22  112</br>\n",
    "             ADVP: precision:  68.73%; recall:  50.25%; FB1:  58.06  291</br>\n",
    "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0</br>\n",
    "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0</br>\n",
    "               NP: precision:  75.00%; recall:  81.05%; FB1:  77.91  6740</br>\n",
    "               PP: precision:  91.65%; recall:  88.16%; FB1:  89.87  2348</br>\n",
    "              PRT: precision:  64.86%; recall:  53.33%; FB1:  58.54  37</br>\n",
    "             SBAR: precision:  78.91%; recall:  42.62%; FB1:  55.34  128</br>\n",
    "               VP: precision:  68.66%; recall:  70.92%; FB1:  69.77  2380</br>\n",
    "dev.out score: 76.9848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
